{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8483151",
   "metadata": {},
   "source": [
    "### Exemplo - Tensortrade\n",
    "\n",
    "- Dados diários de OHLCV obtidos via API da Yahoo! Finance para uma ação específica.\n",
    "- Função de recompensa anexada ao esquema de ações (profit based).\n",
    "- O agente transfere todos os recursos de uma carteira para ativos e vice-versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas_ta as ta\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
    "from tensortrade.feed.core import Stream, DataFeed\n",
    "\n",
    "from gym.spaces import Discrete\n",
    "from tensortrade.env.default.actions import TensorTradeActionScheme\n",
    "from tensortrade.env.generic import ActionScheme, TradingEnv\n",
    "from tensortrade.core import Clock\n",
    "from tensortrade.oms.instruments import ExchangePair, Instrument\n",
    "from tensortrade.oms.wallets import Portfolio\n",
    "from tensortrade.oms.orders import (\n",
    "    Order,\n",
    "    proportion_order,\n",
    "    TradeSide,\n",
    "    TradeType\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensortrade.env.generic import Renderer\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c27d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRL = Instrument(\"BRL\", 2, \"Brazilian Currency\")\n",
    "ASSET = Instrument(\"ASSET\", 2, \"Asset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1a58b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407fcd25",
   "metadata": {},
   "source": [
    "### Action Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSH(TensorTradeActionScheme):\n",
    "    \"\"\"The ActionScheme interprets and applies the agent’s actions to the environment.\"\"\"\n",
    "\n",
    "    registered_name = \"bsh\"\n",
    "\n",
    "    def __init__(self, cash: 'Wallet', asset: 'Wallet'):\n",
    "        super().__init__()\n",
    "        self.cash = cash\n",
    "        self.asset = asset\n",
    "\n",
    "        self.listeners = []\n",
    "        self.action = 0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return Discrete(2)\n",
    "\n",
    "    def attach(self, listener):\n",
    "        self.listeners += [listener]\n",
    "        return self\n",
    "\n",
    "    def get_orders(self,\n",
    "                   action: int,\n",
    "                   portfolio: 'Portfolio'):\n",
    "        \n",
    "        order = None\n",
    "\n",
    "        if abs(action - self.action) > 0:\n",
    "            src = self.cash if self.action == 0 else self.asset\n",
    "            tgt = self.asset if self.action == 0 else self.cash\n",
    "\n",
    "            if src == self.cash:\n",
    "                # Calculates proportional order size (n lots of 100 shares)\n",
    "                lot_size = 100.00\n",
    "                current_price = float(portfolio.exchange_pairs[0].price)\n",
    "                source_balance = src.balance.as_float()\n",
    "\n",
    "                qtd_assets = source_balance / (lot_size * current_price)\n",
    "\n",
    "                num_shares = int(qtd_assets - (qtd_assets % 10)) * lot_size\n",
    "\n",
    "                proportional_lot_size = (num_shares * current_price) / source_balance\n",
    "            else:\n",
    "                proportional_lot_size = 1.0\n",
    "\n",
    "            print('--' * 50)\n",
    "            \n",
    "            if src == self.cash:\n",
    "                print ('CASH TO ASSET')\n",
    "                print('Source Balance: ', src.balance.as_float())\n",
    "                print('Target Balance: ', tgt.balance.as_float())\n",
    "                print('Proportional Lot Size', proportional_lot_size)\n",
    "                print('Current Price: ', float(portfolio.exchange_pairs[0].price))\n",
    "                print('# Shares: ', num_shares)\n",
    "                print('Current Price x # Shares: ', num_shares * float(portfolio.exchange_pairs[0].price))\n",
    "            else:\n",
    "                print ('ASSET TO CASH')\n",
    "                print('Source Balance: ', src.balance.as_float())\n",
    "                print('Target Balance: ', tgt.balance.as_float())\n",
    "                print('Proportional Lot Size', proportional_lot_size)\n",
    "\n",
    "            order = proportion_order(\n",
    "                        portfolio, \n",
    "                        src, \n",
    "                        tgt, \n",
    "                        proportional_lot_size\n",
    "                    )\n",
    "\n",
    "            self.action = action\n",
    "\n",
    "        for listener in self.listeners:\n",
    "            listener.on_action(action)\n",
    "\n",
    "        return [order]\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.action = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7920cf8b",
   "metadata": {},
   "source": [
    "### Reward Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6d815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBR(TensorTradeRewardScheme):\n",
    "\n",
    "    \"\"\" Position-based reward scheme (PBR).\n",
    "    \n",
    "    The RewardScheme computes the reward for \n",
    "    each time step based on the agent’s performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    registered_name = \"pbr\"\n",
    "\n",
    "    def __init__(self, price: 'Stream'):\n",
    "        super().__init__()\n",
    "        self.position = -1\n",
    "\n",
    "        r = Stream.sensor(price, lambda p: p.value, dtype=\"float\").diff()\n",
    "        position = Stream.sensor(self, lambda rs: rs.position, dtype=\"float\")\n",
    "\n",
    "        reward = (r * position).fillna(0).rename(\"reward\")\n",
    "\n",
    "        self.feed = DataFeed([reward])\n",
    "        self.feed.compile()\n",
    "\n",
    "    def on_action(self, action: int):\n",
    "        self.position = -1 if action == 0 else 1\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio'):\n",
    "        return self.feed.next()[\"reward\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = -1\n",
    "        self.feed.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cc856f",
   "metadata": {},
   "source": [
    "### Renderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionChangeChart(Renderer):\n",
    "    \"\"\"The Renderer renders a view of the environment and interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        # The Observer generates the next observation for the agent.\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        p = list(history.price)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = p[i]\n",
    "                else:\n",
    "                    sell[i] = p[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"v\", color=\"red\") # BUY\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"green\") # SELL\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "        axs[0].legend(['Price', 'Buys', 'Sells'])\n",
    "\n",
    "        performance_df = pd.DataFrame().from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        performance_df.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515476a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed89af5",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Now in order to use our custom environment in ray we must first write a function that creates an instance of the TradingEnv from a configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_env(config):\n",
    "    \"\"\"Creates Trading Environment. \"\"\"\n",
    "    \n",
    "    ticker = 'PETR4'\n",
    "\n",
    "    # PRICES\n",
    "\n",
    "    yahoo_df = yf.download(\n",
    "        f'{ticker}.SA', \n",
    "        start='2021-01-01', \n",
    "        end='2021-12-31'\n",
    "    )\n",
    "    \n",
    "    y = yahoo_df['Adj Close'].dropna().values\n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"BRL-ASSET\")\n",
    "    \n",
    "    b3 = Exchange(\"B3\", service=execute_order)(p)\n",
    "\n",
    "    # Portfolio\n",
    "    cash = Wallet(b3, 100000 * BRL) # Money\n",
    "    asset = Wallet(b3, 0 * ASSET) # Stocks\n",
    "    \n",
    "    portfolio = Portfolio(BRL, [cash, asset])\n",
    "\n",
    "    # Data\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "    \n",
    "    # Reward\n",
    "    reward_scheme = PBR(\n",
    "        price=p\n",
    "    )\n",
    "    \n",
    "    # Actions\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "    \n",
    "    # Visualization\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(yahoo_df.index)).rename(\"date\"),\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    # Environment\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    \n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_training_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52242db7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d70d1",
   "metadata": {},
   "source": [
    "Now that the environment is registered we can run the training algorithm using the Proximal Policy Optimization (PPO) algorithm implemented in rllib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bccd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 20\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "      \"episode_reward_mean\": 2.5\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": window_size\n",
    "        },\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"eager_tracing\": False,\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 1,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822aba60",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085ee7c",
   "metadata": {},
   "source": [
    "After training is complete, we would now like to get access to the agents policy. We can do that by restoring the agent using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\"    \n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": window_size # We want to look at the last x samples (days)\n",
    "        },\n",
    "        \"framework\": \"tf2\",\n",
    "        \"eager_tracing\": False,\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 1,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "\n",
    "agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf43f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1a0aa",
   "metadata": {},
   "source": [
    "After training is complete, we would now like to get access to the agents policy. We can do that by restoring the agent using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504db828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env = create_training_env({\n",
    "    \"window_size\": window_size\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_single_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c5600",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f04b2",
   "metadata": {},
   "source": [
    "### Validation Set (Out-of-Sampe Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e86b3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_eval_env(config):\n",
    "    y = config[\"y\"]\n",
    "    \n",
    "    p = Stream.source(y, dtype=\"float\").rename(\"BRL-ASSET\")\n",
    "\n",
    "    b3 = Exchange(\"B3\", service=execute_order)(p)\n",
    "\n",
    "    cash = Wallet(b3, 100000 * BRL)\n",
    "    asset = Wallet(b3, 0 * ASSET)\n",
    "\n",
    "    portfolio = Portfolio(BRL, [cash, asset])\n",
    "\n",
    "    feed = DataFeed([\n",
    "        p,\n",
    "        p.rolling(window=10).mean().rename(\"fast\"),\n",
    "        p.rolling(window=50).mean().rename(\"medium\"),\n",
    "        p.rolling(window=100).mean().rename(\"slow\"),\n",
    "        p.log().diff().fillna(0).rename(\"lr\")\n",
    "    ])\n",
    "\n",
    "    reward_scheme = PBR(\n",
    "        price=p\n",
    "    )\n",
    "\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(y, dtype=\"float\").rename(\"price\"),\n",
    "        Stream.sensor(action_scheme, lambda s: s.action, dtype=\"float\").rename(\"action\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=PositionChangeChart(),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.6\n",
    "    )\n",
    "    return environment, portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the environment\n",
    "env, portfolio = create_eval_env({\n",
    "    \"window_size\": window_size,\n",
    "    \"y\": yf.download(f'PETR4.SA', start='2022-01-01', end='2022-04-01')['Adj Close'].dropna().values\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_single_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "env.render()\n",
    "\n",
    "portfolio.ledger.as_frame().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2b429",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f65a30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "portfolio.ledger.as_frame().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185cbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(portfolio.performance)\n",
    "\n",
    "new_column_list = list(yf.download(f'PETR4.SA', start='2022-01-01', end='2022-04-01').index.astype(str).values)\n",
    "\n",
    "assert len(new_column_list) == df.shape[1]\n",
    "\n",
    "df.set_axis(new_column_list, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c5a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evaluation_results(df):\n",
    "    \"\"\"Plot Evaluation Set Results.\"\"\"\n",
    "    fig, ax = plt.subplots(3, sharex=True, figsize=(15,6))\n",
    "    \n",
    "    # Net Worth\n",
    "    df.loc[\"net_worth\"].plot(ax=ax[0])\n",
    "    ax[0].set_title('Net Worth')\n",
    "    ax[0].set_ylabel(\"Cash (R$)\")\n",
    "    #ax[0].set_xlabel(\"Valores em X\")\n",
    "    ax[0].axhline(100000, linestyle='dashed', color='black')\n",
    "    ax[0].legend(['Current Net Worth', 'Initial Net Worth'])\n",
    "    \n",
    "    # Asset Price\n",
    "    df.loc[\"B3:/BRL-ASSET\"].plot(ax=ax[1])\n",
    "    ax[1].set_title('Asset Price')\n",
    "    ax[1].set_ylabel(\"Price (R$)\")\n",
    "    ax[1].legend(['Current Share Price'])\n",
    "    \n",
    "    # Positions\n",
    "    df.loc[\"B3:/ASSET:/total\"].plot(ax=ax[2])\n",
    "    ax[2].set_title('Positions')\n",
    "    ax[2].set_ylabel(\"Shares (#)\")\n",
    "    ax[2].set_xlabel(\"Datetime\")\n",
    "    ax[2].legend(['Current Position'])\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluation_results(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b475db",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

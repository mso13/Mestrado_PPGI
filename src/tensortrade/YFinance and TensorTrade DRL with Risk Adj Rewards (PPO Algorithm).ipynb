{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pandas_ta as ta\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange,ExchangeOptions\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "from tensortrade.env.default.rewards import TensorTradeRewardScheme\n",
    "from tensortrade.feed.core import Stream, DataFeed\n",
    "\n",
    "from gym.spaces import Discrete\n",
    "from tensortrade.env.default.actions import TensorTradeActionScheme\n",
    "from tensortrade.env.generic import ActionScheme, TradingEnv\n",
    "from tensortrade.core import Clock\n",
    "from tensortrade.oms.instruments import ExchangePair, Instrument\n",
    "from tensortrade.oms.wallets import Portfolio\n",
    "from tensortrade.oms.orders import (\n",
    "    Order,\n",
    "    proportion_order,\n",
    "    TradeSide,\n",
    "    TradeType\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensortrade.env.generic import Renderer\n",
    "\n",
    "import ray.rllib.agents.ppo as ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd2fb17",
   "metadata": {},
   "source": [
    "### Renderer (Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e0a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionChangeChart(Renderer):\n",
    "    \"\"\"The Renderer renders a view of the environment and interactions.\"\"\"\n",
    "    \n",
    "    def __init__(self, color: str = \"orange\"):\n",
    "        self.color = \"orange\"\n",
    "\n",
    "    def render(self, env, **kwargs):\n",
    "        # The Observer generates the next observation for the agent.\n",
    "        history = pd.DataFrame(env.observer.renderer_history)\n",
    "\n",
    "        actions = list(history.action)\n",
    "        p = list(history.price)\n",
    "\n",
    "        buy = {}\n",
    "        sell = {}\n",
    "\n",
    "        for i in range(len(actions) - 1):\n",
    "            a1 = actions[i]\n",
    "            a2 = actions[i + 1]\n",
    "\n",
    "            if a1 != a2:\n",
    "                if a1 == 0 and a2 == 1:\n",
    "                    buy[i] = p[i]\n",
    "                else:\n",
    "                    sell[i] = p[i]\n",
    "\n",
    "        buy = pd.Series(buy)\n",
    "        sell = pd.Series(sell)\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "        fig.suptitle(\"Performance\")\n",
    "\n",
    "        axs[0].plot(np.arange(len(p)), p, label=\"price\", color=self.color)\n",
    "        axs[0].scatter(buy.index, buy.values, marker=\"v\", color=\"red\") # BUY\n",
    "        axs[0].scatter(sell.index, sell.values, marker=\"^\", color=\"green\") # SELL\n",
    "        axs[0].set_title(\"Trading Chart\")\n",
    "        axs[0].legend(['Price', 'Buys', 'Sells'])\n",
    "\n",
    "        performance_df = pd.DataFrame().from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "        performance_df.plot(ax=axs[1])\n",
    "        axs[1].set_title(\"Net Worth\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515476a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c333762d",
   "metadata": {},
   "source": [
    "### DRL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed89af5",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Now in order to use our custom environment in ray we must first write a function that creates an instance of the TradingEnv from a configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bee78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_datasets(ticker, train_test_split):\n",
    "    \"\"\"Get Yahoo! Finance Data for Train/Test Splits.\"\"\"\n",
    "\n",
    "    yf_ticker = yf.Ticker(ticker=f'{ticker}.SA')\n",
    "\n",
    "    df = yf_ticker.history(period='2y', interval='1h')\n",
    "    df.drop(['Dividends', 'Stock Splits'], axis=1, inplace=True)\n",
    "    df[\"Volume\"] = df[\"Volume\"].fillna(0).astype(int)\n",
    "    df.ta.log_return(append=True, length=16)\n",
    "    df.ta.rsi(append=True, length=14)\n",
    "    df.ta.macd(append=True, fast=12, slow=26)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    next_day = datetime.strptime(train_test_split, '%Y-%m-%d')\n",
    "    next_day = next_day + timedelta(days=1)\n",
    "    next_day = next_day.strftime('%Y-%m-%d')\n",
    "\n",
    "    df_training = df.loc[:train_test_split].copy()\n",
    "    df_evaluation = df.loc[next_day:].copy()\n",
    "\n",
    "    df_training.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "    df_evaluation.rename(columns={'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)\n",
    "\n",
    "    df_training.dropna().to_csv('../../data/testing/training.csv', index=True)\n",
    "    df_evaluation.dropna().to_csv('../../data/testing/evaluation.csv', index=True)\n",
    "\n",
    "    return df_training, df_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f800b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train, df_test = generate_train_test_datasets('PETR4', '2022-01-03')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67535976",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBR(TensorTradeRewardScheme):\n",
    "\n",
    "    \"\"\" Position-based reward scheme (PBR).\n",
    "    \n",
    "    The RewardScheme computes the reward for \n",
    "    each time step based on the agent’s performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    registered_name = \"pbr\"\n",
    "\n",
    "    def __init__(self, price: 'Stream'):\n",
    "        super().__init__()\n",
    "        self.position = -1\n",
    "\n",
    "        r = Stream.sensor(price, lambda p: p.value, dtype=\"float\").diff()\n",
    "        position = Stream.sensor(self, lambda rs: rs.position, dtype=\"float\")\n",
    "\n",
    "        reward = (r * position).fillna(0).rename(\"reward\")\n",
    "\n",
    "        self.feed = DataFeed([reward])\n",
    "        self.feed.compile()\n",
    "\n",
    "    def on_action(self, action: int):\n",
    "        self.position = -1 if action == 0 else 1\n",
    "\n",
    "    def get_reward(self, portfolio: 'Portfolio'):\n",
    "        return self.feed.next()[\"reward\"]\n",
    "\n",
    "    def reset(self):\n",
    "        self.position = -1\n",
    "        self.feed.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f35cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSH(TensorTradeActionScheme):\n",
    "    \"\"\"The ActionScheme interprets and applies the agent’s actions to the environment.\"\"\"\n",
    "\n",
    "    registered_name = \"bsh\"\n",
    "\n",
    "    def __init__(self, cash: 'Wallet', asset: 'Wallet'):\n",
    "        super().__init__()\n",
    "        self.cash = cash\n",
    "        self.asset = asset\n",
    "\n",
    "        self.listeners = []\n",
    "        self.action = 0\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return Discrete(2)\n",
    "\n",
    "    def attach(self, listener):\n",
    "        self.listeners += [listener]\n",
    "        return self\n",
    "\n",
    "    def get_orders(self,\n",
    "                   action: int,\n",
    "                   portfolio: 'Portfolio'):\n",
    "        \n",
    "        order = None\n",
    "\n",
    "        if abs(action - self.action) > 0:\n",
    "            src = self.cash if self.action == 0 else self.asset\n",
    "            tgt = self.asset if self.action == 0 else self.cash\n",
    "\n",
    "            if src == self.cash:\n",
    "                # Calculates proportional order size (n lots of 100 shares)\n",
    "                lot_size = 100.00\n",
    "                current_price = float(portfolio.exchange_pairs[0].price)\n",
    "                source_balance = src.balance.as_float()\n",
    "\n",
    "                qtd_assets = source_balance / (lot_size * current_price)\n",
    "\n",
    "                num_shares = int(qtd_assets - (qtd_assets % 10)) * lot_size\n",
    "\n",
    "                proportional_lot_size = (num_shares * current_price) / source_balance\n",
    "            else:\n",
    "                proportional_lot_size = 1.0\n",
    "\n",
    "            print('--' * 50)\n",
    "            \n",
    "            if src == self.cash:\n",
    "                print ('CASH TO ASSET')\n",
    "                print('Source Balance: ', src.balance.as_float())\n",
    "                print('Target Balance: ', tgt.balance.as_float())\n",
    "                print('Proportional Lot Size', proportional_lot_size)\n",
    "                print('Current Price: ', float(portfolio.exchange_pairs[0].price))\n",
    "                print('# Shares: ', num_shares)\n",
    "                print('Current Price x # Shares: ', num_shares * float(portfolio.exchange_pairs[0].price))\n",
    "            else:\n",
    "                print ('ASSET TO CASH')\n",
    "                print('Source Balance: ', src.balance.as_float())\n",
    "                print('Target Balance: ', tgt.balance.as_float())\n",
    "                print('Proportional Lot Size', proportional_lot_size)\n",
    "\n",
    "            order = proportion_order(\n",
    "                        portfolio, \n",
    "                        src, \n",
    "                        tgt, \n",
    "                        proportional_lot_size\n",
    "                    )\n",
    "\n",
    "            self.action = action\n",
    "\n",
    "        for listener in self.listeners:\n",
    "            listener.on_action(action)\n",
    "\n",
    "        return [order]\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.action = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c5962",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_env(config):\n",
    "    \"\"\"Creates Trading Environment. \"\"\"\n",
    "    \n",
    "    # 1. Data and TensorTrade Objects\n",
    "    # dataset = pd.read_csv(filepath_or_buffer=\"../../data/testing/training.csv\", \n",
    "    #                       parse_dates=True).fillna(method='backfill').fillna(method='ffill')\n",
    "\n",
    "    # dataset.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "\n",
    "    dataset = pd.read_csv('../../data/market/ohlcv_daily_TA/PETR3_train.csv')\n",
    "    \n",
    "    # Price Series\n",
    "    price = Stream.source(list(dataset[\"close\"]), dtype=\"float\").rename(\"BRL-ASSETS\")\n",
    "    \n",
    "    b3_commission = 0.0035\n",
    "    b3_options = ExchangeOptions(commission=b3_commission)\n",
    "    b3_exchange = Exchange(\"B3\", service=execute_order, options=b3_options)(price)\n",
    "    \n",
    "    # Instruments\n",
    "    BRL = Instrument(\"BRL\", 2, \"Brazilian Currency\")\n",
    "    ASSETS = Instrument(\"ASSETS\", 2, \"Assets\")\n",
    "\n",
    "    # Portfolio\n",
    "    cash = Wallet(b3_exchange, 100000 * BRL) # Money\n",
    "    asset = Wallet(b3_exchange, 0 * ASSETS) # Stocks/Assets\n",
    "    \n",
    "    portfolio = Portfolio(BRL, [cash, asset])\n",
    "    \n",
    "    features = []\n",
    "    for c in dataset.columns[1:]:\n",
    "        s = Stream.source(list(dataset[c]), dtype=\"float\").rename(dataset[c].name)\n",
    "        features += [s]\n",
    "    feed = DataFeed(features)\n",
    "    feed.compile()\n",
    "    \n",
    "    # 2. Rewards\n",
    "\n",
    "    # Rewards - Risk Adjusted\n",
    "    # reward_scheme = default.rewards.RiskAdjustedReturns(\n",
    "    #     return_algorithm='sharpe',\n",
    "    #     risk_free_rate=0,\n",
    "    #     window_size= 7 * 5\n",
    "    # )\n",
    "\n",
    "    # Reward\n",
    "    reward_scheme = PBR(\n",
    "        price=price\n",
    "    )\n",
    "    \n",
    "    # Actions\n",
    "    action_scheme = BSH(\n",
    "        cash=cash,\n",
    "        asset=asset\n",
    "    ).attach(reward_scheme)\n",
    "    \n",
    "\n",
    "    # Rewards - Simple Profit\n",
    "    # reward_scheme = default.rewards.SimpleProfit()\n",
    "    \n",
    "    # 3. Actions\n",
    "\n",
    "    # action_scheme = default.actions.ManagedRiskOrders(\n",
    "    #     stop=[0.05],\n",
    "    #     take=[0.075],\n",
    "    #     min_order_pct=0.5\n",
    "    # )\n",
    "\n",
    "    # A discrete action scheme that determines actions based on managing risk\n",
    "    # action_scheme = default.actions.BSH(cash, asset)\n",
    "\n",
    "    # 4. Visualization\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(dataset['date'])).rename(\"date\"),\n",
    "        Stream.source(list(dataset[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
    "        Stream.source(list(dataset[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
    "        Stream.source(list(dataset[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
    "        Stream.source(list(dataset[\"close\"]), dtype=\"float\").rename(\"close\"),\n",
    "        Stream.source(list(dataset[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
    "    ])\n",
    "\n",
    "    # 5. Environment\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=default.renderers.PlotlyTradingChart(display=True, auto_open_html=False, save_format=\"png\"),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.4\n",
    "    )\n",
    "    \n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_training_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52242db7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d70d1",
   "metadata": {},
   "source": [
    "Now that the environment is registered we can run the training algorithm using the Proximal Policy Optimization (PPO) algorithm implemented in rllib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bccd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 14\n",
    "\n",
    "env_config_training = {\n",
    "    # We want to look at the last 14 samples (hours)\n",
    "    \"window_size\": window_size, # hours * days\n",
    "    # And calculate reward based on the actions taken in the next 7 hours\n",
    "    #\"reward_window_size\": 7,\n",
    "    # If it goes past 10% loss during the iteration, we don't want to waste time on a \"loser\".\n",
    "    \"max_allowed_loss\": 0.10,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    run_or_experiment=\"PPO\", # Builtin PPO agent in RLLib\n",
    "    name=\"BrazilianStockTrading\",\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    stop={\n",
    "      \"episode_reward_mean\": 0.03\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": env_config_training,\n",
    "        \"log_level\": \"WARNING\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"eager_tracing\": True,\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1, \n",
    "        \"num_gpus\": 1,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0.5,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822aba60",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085ee7c",
   "metadata": {},
   "source": [
    "After training is complete, we would now like to get access to the agents policy. We can do that by restoring the agent using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\"    \n",
    ")\n",
    "\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": env_config_training,\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 1,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0.5,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": [256, 256], # Hyperparameter grid search defined above\n",
    "        },\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf43f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1a0aa",
   "metadata": {},
   "source": [
    "After training is complete, we would now like to get access to the agents policy. We can do that by restoring the agent using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504db828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore agent\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_training_env({\n",
    "    \"window_size\": window_size\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_single_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c5600",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f04b2",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e86b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_env(config):\n",
    "    \"\"\"Creates the Evaluation Environment.\"\"\"\n",
    "    # dataset = pd.read_csv(filepath_or_buffer=\"../../data/testing/evaluation.csv\", \n",
    "    #                       parse_dates=True).fillna(method='backfill').fillna(method='ffill')\n",
    "\n",
    "    # dataset.rename(columns={'Unnamed: 0': 'date'}, inplace=True)\n",
    "\n",
    "    dataset = pd.read_csv('../../data/market/ohlcv_daily_TA/PETR3_test.csv')\n",
    "\n",
    "    # Price Series\n",
    "    price = Stream.source(list(dataset[\"close\"]), dtype=\"float\").rename(\"BRL-ASSETS\")\n",
    "    \n",
    "    b3_commission = 0.0035\n",
    "    b3_options = ExchangeOptions(commission=b3_commission)\n",
    "    b3_exchange = Exchange(\"B3\", service=execute_order, options=b3_options)(price)\n",
    "    \n",
    "    # Instruments\n",
    "    BRL = Instrument(\"BRL\", 2, \"Brazilian Currency\")\n",
    "    ASSETS = Instrument(\"ASSETS\", 2, \"Assets\")\n",
    "\n",
    "    # Portfolio\n",
    "    cash = Wallet(b3_exchange, 100000 * BRL) # Money\n",
    "    asset = Wallet(b3_exchange, 0 * ASSETS) # Stocks/Assets\n",
    "    \n",
    "    portfolio = Portfolio(BRL, [cash, asset])\n",
    "    \n",
    "    features = []\n",
    "    for c in dataset.columns[1:]:\n",
    "        s = Stream.source(list(dataset[c]), dtype=\"float\").rename(dataset[c].name)\n",
    "        features += [s]\n",
    "    feed = DataFeed(features)\n",
    "    feed.compile()\n",
    "    \n",
    "    # Rewards - Risk Adjusted\n",
    "    # reward_scheme = default.rewards.RiskAdjustedReturns(\n",
    "    #     return_algorithm='sharpe',\n",
    "    #     risk_free_rate=0,\n",
    "    #     window_size= 7 * 5\n",
    "    # )\n",
    "\n",
    "    # Rewards - Simple Profit\n",
    "    reward_scheme = default.rewards.SimpleProfit()\n",
    "    \n",
    "    # Actions\n",
    "    action_scheme = default.actions.ManagedRiskOrders(\n",
    "        stop=[0.05],\n",
    "        take=[0.075],\n",
    "        min_order_pct=0.5\n",
    "    )\n",
    "\n",
    "    # A discrete action scheme that determines actions based on managing risk\n",
    "    # action_scheme = default.actions.ManagedRiskOrders()\n",
    "    \n",
    "    # Visualization\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(dataset['date'])).rename(\"date\"),\n",
    "        Stream.source(list(dataset[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
    "        Stream.source(list(dataset[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
    "        Stream.source(list(dataset[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
    "        Stream.source(list(dataset[\"close\"]), dtype=\"float\").rename(\"close\"),\n",
    "        Stream.source(list(dataset[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=default.renderers.PlotlyTradingChart(display=True, auto_open_html=False, save_format=\"png\"),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.4\n",
    "    )\n",
    "    \n",
    "    return environment, portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the evaluation environment\n",
    "env, portfolio = create_eval_env({\n",
    "    \"window_size\": window_size\n",
    "})\n",
    "\n",
    "# 2. Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_single_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "env.render()\n",
    "\n",
    "portfolio.ledger.as_frame().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.ledger.as_frame().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(portfolio.performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc603aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dff236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"B3:/BRL-ASSETS\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a073c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"B3:/ASSETS:/worth\"].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2b429",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90114e7e",
   "metadata": {},
   "source": [
    "### Net Worth Analysis (with Quantstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantstats as qs\n",
    "\n",
    "# extend pandas functionality with metrics, etc.\n",
    "qs.extend_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30036c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(filepath_or_buffer=\"../../data/testing/evaluation.csv\", \n",
    "                          parse_dates=True).fillna(method='backfill').fillna(method='ffill')\n",
    "\n",
    "dataset.rename(columns={'Unnamed: 0': 'Date'}, inplace=True)\n",
    "\n",
    "net_worth = df.loc[\"net_worth\"].rename('Close')\n",
    "net_worth.index = dataset['Date'].loc[1:]\n",
    "net_worth.index = pd.to_datetime(net_worth.index)\n",
    "\n",
    "net_worth = net_worth.resample('D').last()\n",
    "net_worth.index = net_worth.index.date\n",
    "net_worth.index = net_worth.index.rename('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a553b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_returns = net_worth.pct_change().dropna()\n",
    "net_returns.index = pd.to_datetime(net_returns.index)\n",
    "net_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ead5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sharpe ratio\n",
    "qs.stats.sharpe(net_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overall performance\n",
    "qs.plots.snapshot(net_returns, title=f'Evaluation Set Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime to date (keep end of day result)\n",
    "qs.reports.html(net_returns, \"^BVSP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319bf23",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

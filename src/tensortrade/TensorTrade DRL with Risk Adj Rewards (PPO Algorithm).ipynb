{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3387369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import math\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Ray Imports\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# TT Imports\n",
    "import tensortrade.env.default as default\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.exchanges import Exchange,ExchangeOptions\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "from tensortrade.feed.core import Stream, DataFeed\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "from tensortrade.oms.wallets import Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9515476a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c333762d",
   "metadata": {},
   "source": [
    "### DRL Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed89af5",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "Now in order to use our custom environment in ray we must first write a function that creates an instance of the TradingEnv from a configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b8095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_env(config):\n",
    "    \"\"\"Creates Trading Environment. \"\"\"\n",
    "    \n",
    "    # 1. Data and TensorTrade Objects\n",
    "    # dataset = pd.read_csv('../../data/market/ohlcv_daily_TA/PETR3_train.csv')\n",
    "    dataset = pd.read_csv('../../data/contextual_data_market_and_text/daily/PETR3_train.csv')\n",
    "    \n",
    "    # Price Series\n",
    "    price = Stream.source(list(dataset[\"close\"]), dtype=\"float\").rename(\"BRL-ASSETS\")\n",
    "    \n",
    "    b3_commission = 0.0035\n",
    "    b3_options = ExchangeOptions(commission=b3_commission)\n",
    "    b3_exchange = Exchange( name=\"B3\", \n",
    "                            service=execute_order, \n",
    "                            options=b3_options)(price)\n",
    "    \n",
    "    # Instruments\n",
    "    BRL = Instrument(\"BRL\", 2, \"Brazilian Currency\")\n",
    "    ASSETS = Instrument(\"ASSETS\", 2, \"Assets\")\n",
    "\n",
    "    # Portfolio\n",
    "    cash = Wallet(b3_exchange, 100000 * BRL) # Money\n",
    "    asset = Wallet(b3_exchange, 0 * ASSETS) # Stocks/Assets\n",
    "    \n",
    "    portfolio = Portfolio(BRL, [cash, asset])\n",
    "    \n",
    "    features = []\n",
    "    for c in dataset.columns[1:]:\n",
    "        s = Stream.source(list(dataset[c]), dtype=\"float\").rename(dataset[c].name)\n",
    "        features += [s]\n",
    "    print (features)\n",
    "    feed = DataFeed(features)\n",
    "    feed.compile()\n",
    "    \n",
    "    # 2. Rewards\n",
    "\n",
    "    # Rewards - Risk Adjusted Returns\n",
    "    # reward_scheme = default.rewards.RiskAdjustedReturns(\n",
    "    #     return_algorithm='sharpe',\n",
    "    #     risk_free_rate=0,\n",
    "    #     window_size= 7 * 5\n",
    "    # )\n",
    "    \n",
    "    # Rewards - Simple Profit\n",
    "    reward_scheme = default.rewards.SimpleProfit()\n",
    "    \n",
    "    # 3. Actions - Managed Risk Orders\n",
    "    action_scheme = default.actions.ManagedRiskOrders(\n",
    "        stop=[0.05],\n",
    "        take=[0.075],\n",
    "        min_order_pct=0.5\n",
    "    )\n",
    "\n",
    "    # 4. Visualization\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(dataset['date'])).rename(\"date\"),\n",
    "        Stream.source(list(dataset[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
    "        Stream.source(list(dataset[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
    "        Stream.source(list(dataset[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
    "        Stream.source(list(dataset[\"close\"]), dtype=\"float\").rename(\"close\"),\n",
    "        Stream.source(list(dataset[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
    "    ])\n",
    "\n",
    "    # 5. Environment\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=default.renderers.PlotlyTradingChart(display=True, auto_open_html=False, save_format=\"png\"),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.4\n",
    "    )\n",
    "    \n",
    "    return environment\n",
    "\n",
    "register_env(\"TradingEnv\", create_training_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52242db7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d70d1",
   "metadata": {},
   "source": [
    "Now that the environment is registered we can run the training algorithm using the Proximal Policy Optimization (PPO) algorithm implemented in rllib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bccd9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "\n",
    "env_config_training = {\n",
    "    # We want to look at the last N samples (days or hours)\n",
    "    \"window_size\": window_size, # hours * days\n",
    "    # And calculate reward based on the actions taken in the next 7 hours\n",
    "    \"reward_window_size\": math.ceil(window_size / 2),\n",
    "    # If it goes past 10% loss during the iteration, we don't want to waste time on a \"loser\".\n",
    "    \"max_allowed_loss\": 0.15,\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    run_or_experiment=\"PPO\",\n",
    "    name=\"MyExperiment\",\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    stop={\n",
    "      \"training_iteration\": 15\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"TradingEnv\",\n",
    "        \"env_config\": {\n",
    "            \"window_size\": window_size\n",
    "        },\n",
    "        \"log_level\": \"WARNING\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"eager_tracing\": True,\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0.5,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    },\n",
    "\n",
    "    checkpoint_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822aba60",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085ee7c",
   "metadata": {},
   "source": [
    "After training is complete, we would now like to get access to the agents policy. We can do that by restoring the agent using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b2eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\"    \n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config={\n",
    "        \"env_config\": {\n",
    "            \"window_size\": window_size # We want to look at the last x samples (days)\n",
    "        },\n",
    "        \"framework\": \"tf2\",\n",
    "        \"eager_tracing\": False,\n",
    "        \"log_level\": \"DEBUG\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1,\n",
    "        \"num_gpus\": 0,\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": 8e-6,\n",
    "        \"lr_schedule\": [\n",
    "            [0, 1e-1],\n",
    "            [int(1e2), 1e-2],\n",
    "            [int(1e3), 1e-3],\n",
    "            [int(1e4), 1e-4],\n",
    "            [int(1e5), 1e-5],\n",
    "            [int(1e6), 1e-6],\n",
    "            [int(1e7), 1e-7]\n",
    "        ],\n",
    "        \"gamma\": 0.5,\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"lambda\": 0.72,\n",
    "        \"vf_loss_coeff\": 0.5,\n",
    "        \"entropy_coeff\": 0.01\n",
    "    }\n",
    ")\n",
    "\n",
    "agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf43f2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1a0aa",
   "metadata": {},
   "source": [
    "After training is complete, we would now like to get access to the agents policy. We can do that by restoring the agent using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504db828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore agent\n",
    "agent.restore(checkpoint_path)\n",
    "\n",
    "# Instantiate the environment\n",
    "env = create_training_env({\n",
    "    \"window_size\": window_size\n",
    "})\n",
    "\n",
    "# Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_single_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50c5600",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f04b2",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e86b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eval_env(config):\n",
    "    \"\"\"Creates the Evaluation Environment.\"\"\"\n",
    "    #dataset = pd.read_csv('../../data/market/ohlcv_daily_TA/PETR3_test.csv')\n",
    "    dataset = pd.read_csv('../../data/contextual_data_market_and_text/daily/PETR3_test.csv')\n",
    "\n",
    "    # Price Series\n",
    "    price = Stream.source(list(dataset[\"close\"]), dtype=\"float\").rename(\"BRL-ASSETS\")\n",
    "    \n",
    "    b3_commission = 0.0035\n",
    "    b3_options = ExchangeOptions(commission=b3_commission)\n",
    "    b3_exchange = Exchange(\"B3\", service=execute_order, options=b3_options)(price)\n",
    "    \n",
    "    # Instruments\n",
    "    BRL = Instrument(\"BRL\", 2, \"Brazilian Currency\")\n",
    "    ASSETS = Instrument(\"ASSETS\", 2, \"Assets\")\n",
    "\n",
    "    # Portfolio\n",
    "    cash = Wallet(b3_exchange, 100000 * BRL) # Money\n",
    "    asset = Wallet(b3_exchange, 0 * ASSETS) # Stocks/Assets\n",
    "    \n",
    "    portfolio = Portfolio(BRL, [cash, asset])\n",
    "    \n",
    "    features = []\n",
    "    for c in dataset.columns[1:]:\n",
    "        s = Stream.source(list(dataset[c]), dtype=\"float\").rename(dataset[c].name)\n",
    "        features += [s]\n",
    "    print (features)\n",
    "    feed = DataFeed(features)\n",
    "    feed.compile()\n",
    "    \n",
    "    # Rewards - Risk Adjusted Returns\n",
    "    # reward_scheme = default.rewards.RiskAdjustedReturns(\n",
    "    #     return_algorithm='sharpe',\n",
    "    #     risk_free_rate=0,\n",
    "    #     window_size= 7 * 5\n",
    "    # )\n",
    "\n",
    "    # Rewards - Simple Profit\n",
    "    reward_scheme = default.rewards.RiskAdjustedReturns()#SimpleProfit()\n",
    "    \n",
    "    # Actions\n",
    "    action_scheme = default.actions.ManagedRiskOrders(\n",
    "        stop=[0.05],\n",
    "        take=[0.075],\n",
    "        min_order_pct=0.5\n",
    "    )\n",
    "\n",
    "    # A discrete action scheme that determines actions based on managing risk\n",
    "    # action_scheme = default.actions.ManagedRiskOrders()\n",
    "    \n",
    "    # Visualization\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(dataset['date'])).rename(\"date\"),\n",
    "        Stream.source(list(dataset[\"open\"]), dtype=\"float\").rename(\"open\"),\n",
    "        Stream.source(list(dataset[\"high\"]), dtype=\"float\").rename(\"high\"),\n",
    "        Stream.source(list(dataset[\"low\"]), dtype=\"float\").rename(\"low\"),\n",
    "        Stream.source(list(dataset[\"close\"]), dtype=\"float\").rename(\"close\"),\n",
    "        Stream.source(list(dataset[\"volume\"]), dtype=\"float\").rename(\"volume\")\n",
    "    ])\n",
    "\n",
    "    environment = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=default.renderers.PlotlyTradingChart(display=True, auto_open_html=False, save_format=\"png\"),\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=0.4\n",
    "    )\n",
    "    \n",
    "    return environment, portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Instantiate the evaluation environment\n",
    "env, portfolio = create_eval_env({\n",
    "    \"window_size\": window_size\n",
    "})\n",
    "\n",
    "# 2. Run until episode ends\n",
    "episode_reward = 0\n",
    "done = False\n",
    "obs = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = agent.compute_single_action(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "    \n",
    "env.render()\n",
    "\n",
    "portfolio.ledger.as_frame().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b164a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio.ledger.as_frame().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(portfolio.performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc603aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.plot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dff236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"B3:/BRL-ASSETS\"].plot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a073c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[\"B3:/ASSETS:/worth\"].plot(figsize=(20,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2b429",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90114e7e",
   "metadata": {},
   "source": [
    "### Net Worth Analysis (with Quantstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db00f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantstats as qs\n",
    "\n",
    "# extend pandas functionality with metrics, etc.\n",
    "qs.extend_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30036c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv('../../data/market/ohlcv_daily_TA/PETR3_test.csv')\n",
    "dataset = pd.read_csv('../../data/contextual_data_market_and_text/daily/PETR3_test.csv')\n",
    "\n",
    "net_worth = df.loc[\"net_worth\"].rename('close')\n",
    "net_worth.index = dataset['date'].loc[:]\n",
    "net_worth.index = pd.to_datetime(net_worth.index)\n",
    "\n",
    "net_worth = net_worth.resample('D').last()\n",
    "net_worth.index = net_worth.index.date\n",
    "net_worth.index = net_worth.index.rename('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a553b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_returns = net_worth.pct_change().dropna()\n",
    "net_returns.index = pd.to_datetime(net_returns.index)\n",
    "net_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ead5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sharpe ratio\n",
    "qs.stats.sharpe(net_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd1dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show overall performance\n",
    "qs.plots.snapshot(net_returns, title=f'Evaluation Set Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime to date (keep end of day result)\n",
    "qs.reports.html(net_returns, \"^BVSP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e319bf23",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

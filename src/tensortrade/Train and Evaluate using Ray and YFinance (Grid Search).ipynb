{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7e8d5e",
   "metadata": {},
   "source": [
    "### Build the training\\evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800ce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance\n",
    "import pandas_ta #noqa\n",
    "\n",
    "TICKER = 'PETR4.SA' # TODO: replace this with your own ticker\n",
    "\n",
    "TRAIN_START_DATE = '2021-02-09' # TODO: replace this with your own start date\n",
    "TRAIN_END_DATE = '2021-09-30' # TODO: replace this with your own end date\n",
    "\n",
    "EVAL_START_DATE = '2021-10-01' # TODO: replace this with your own end date\n",
    "EVAL_END_DATE = '2021-11-12' # TODO: replace this with your own end date\n",
    "\n",
    "yf_ticker = yfinance.Ticker(ticker=TICKER)\n",
    "\n",
    "df_training = yf_ticker.history(start=TRAIN_START_DATE, end=TRAIN_END_DATE, interval='60m')\n",
    "df_training.drop(['Dividends', 'Stock Splits'], axis=1, inplace=True)\n",
    "df_training[\"Volume\"] = df_training[\"Volume\"].astype(int)\n",
    "df_training.ta.log_return(append=True, length=16)\n",
    "df_training.ta.rsi(append=True, length=14)\n",
    "df_training.ta.macd(append=True, fast=12, slow=26)\n",
    "df_training.dropna().to_csv('../../data/training.csv', index=True)\n",
    "\n",
    "df_evaluation = yf_ticker.history(start=EVAL_START_DATE, end=EVAL_END_DATE, interval='60m')\n",
    "df_evaluation.drop(['Dividends', 'Stock Splits'], axis=1, inplace=True)\n",
    "df_evaluation[\"Volume\"] = df_evaluation[\"Volume\"].astype(int)\n",
    "df_evaluation.ta.log_return(append=True, length=16)\n",
    "df_evaluation.ta.rsi(append=True, length=14)\n",
    "df_evaluation.ta.macd(append=True, fast=12, slow=26)\n",
    "df_evaluation.dropna().to_csv('../../data/evaluation.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78da6fd7",
   "metadata": {},
   "source": [
    "### Create the environment build function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43279955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensortrade.feed.core import DataFeed, Stream\n",
    "from tensortrade.oms.instruments import Instrument\n",
    "from tensortrade.oms.exchanges import Exchange, ExchangeOptions\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "import tensortrade.env.default as default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cea28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(config):\n",
    "    dataset = pd.read_csv(filepath_or_buffer=config[\"csv_filename\"], parse_dates=['Datetime']).fillna(method='backfill').fillna(method='ffill')\n",
    "    ttse_commission = 0.0035 # TODO: adjust according to your commission percentage, if present\n",
    "    price = Stream.source(list(dataset[\"Close\"]), dtype=\"float\").rename(\"USD-TTRD\")\n",
    "    ttse_options = ExchangeOptions(commission=ttse_commission)\n",
    "    ttse_exchange = Exchange(\"TTSE\", service=execute_order, options=ttse_options)(price)\n",
    "        \n",
    "    # Instruments, Wallets and Portfolio\n",
    "    USD = Instrument(\"USD\", 2, \"US Dollar\")\n",
    "    TTRD = Instrument(\"TTRD\", 2, \"TensorTrade Corp\")\n",
    "    cash = Wallet(ttse_exchange, 1000 * USD) # This is the starting cash we are going to use\n",
    "    asset = Wallet(ttse_exchange, 0 * TTRD) # And we will start owning 0 stocks of TTRD\n",
    "    portfolio = Portfolio(USD, [cash, asset])\n",
    "    \n",
    "    # Renderer feed\n",
    "    renderer_feed = DataFeed([\n",
    "        Stream.source(list(dataset[\"Datetime\"])).rename(\"date\"),\n",
    "        Stream.source(list(dataset[\"Open\"]), dtype=\"float\").rename(\"open\"),\n",
    "        Stream.source(list(dataset[\"High\"]), dtype=\"float\").rename(\"high\"),\n",
    "        Stream.source(list(dataset[\"Low\"]), dtype=\"float\").rename(\"low\"),\n",
    "        Stream.source(list(dataset[\"Close\"]), dtype=\"float\").rename(\"close\"),\n",
    "        Stream.source(list(dataset[\"Volume\"]), dtype=\"float\").rename(\"volume\")\n",
    "    ])\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    for c in dataset.columns[1:]:\n",
    "        s = Stream.source(list(dataset[c]), dtype=\"float\").rename(dataset[c].name)\n",
    "        features += [s]\n",
    "        \n",
    "    feed = DataFeed(features)\n",
    "    feed.compile()\n",
    "    reward_scheme = default.rewards.SimpleProfit(window_size=config[\"reward_window_size\"])\n",
    "    action_scheme = default.actions.BSH(cash=cash, asset=asset)\n",
    "    env = default.create(\n",
    "        feed=feed,\n",
    "        portfolio=portfolio,\n",
    "        action_scheme=action_scheme,\n",
    "        reward_scheme=reward_scheme,\n",
    "        renderer_feed=renderer_feed,\n",
    "        renderer=[],\n",
    "        window_size=config[\"window_size\"],\n",
    "        max_allowed_loss=config[\"max_allowed_loss\"]\n",
    "    )\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e59d2b0",
   "metadata": {},
   "source": [
    "### Initialize and run Ray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "# Let's define some tuning parameters\n",
    "FC_SIZE = tune.grid_search([[256, 256]]) # Those are the alternatives that ray.tune will try...\n",
    "LEARNING_RATE = tune.grid_search([0.0005]) # ... and they will be combined with these ones ...\n",
    "MINIBATCH_SIZE = tune.grid_search([5]) # ... and these ones, in a cartesian product.\n",
    "\n",
    "# Initialize Ray\n",
    "ray.shutdown()\n",
    "ray.init(\n",
    "    num_cpus=2, \n",
    "    num_gpus=0,\n",
    "    _memory=2000 * 1024 * 1024,\n",
    "    object_store_memory=200 * 1024 * 1024,\n",
    "    _driver_object_store_memory=100 * 1024 * 1024\n",
    ")\n",
    "\n",
    "# Register our environment, specifying which is the environment creation function\n",
    "register_env(\"MyTrainingEnv\", create_env)\n",
    "\n",
    "# Specific configuration keys that will be used during training\n",
    "env_config_training = {\n",
    "    \"window_size\": 14, # We want to look at the last 14 samples (hours)\n",
    "    \"reward_window_size\": 7, # And calculate reward based on the actions taken in the next 7 hours\n",
    "    \"max_allowed_loss\": 0.10, # If it goes past 10% loss during the iteration, we don't want to waste time on a \"loser\".\n",
    "    \"csv_filename\": \"C:/Users/mathe/Documents/Github/Mestrado_PPGI/data/training.csv\" # The variable that will be used to differentiate training and validation datasets\n",
    "}\n",
    "\n",
    "# Specific configuration keys that will be used during evaluation (only the overridden ones)\n",
    "env_config_evaluation = {\n",
    "    \"max_allowed_loss\": 1.00, # During validation runs we want to see how bad it would go. Even up to 100% loss.\n",
    "    \"csv_filename\": \"C:/Users/mathe/Documents/Github/Mestrado_PPGI/data/evaluation.csv\", # The variable that will be used to differentiate training and validation datasets\n",
    "}\n",
    "\n",
    "analysis = tune.run(\n",
    "    run_or_experiment=\"PPO\", # We'll be using the builtin PPO agent in RLLib\n",
    "    name=\"MyExperiment_YF\",\n",
    "    metric='episode_reward_mean',\n",
    "    mode='max',\n",
    "    stop={\n",
    "        \"training_iteration\": 5 # Let's do 5 steps for each hyperparameter combination\n",
    "    },\n",
    "    config={\n",
    "        \"env\": \"MyTrainingEnv\",\n",
    "        \"env_config\": env_config_training, # The dictionary we built before\n",
    "        \"log_level\": \"WARNING\",\n",
    "        \"framework\": \"tf2\",\n",
    "        \"ignore_worker_failures\": True,\n",
    "        \"num_workers\": 1, # One worker per agent. You can increase this but it will run fewer parallel trainings.\n",
    "        \"num_envs_per_worker\": 1,\n",
    "        \"num_gpus\": 0, # I yet have to understand if using a GPU is worth it, for our purposes, but I think it's not. This way you can train on a non-gpu enabled system.\n",
    "        \"clip_rewards\": True,\n",
    "        \"lr\": LEARNING_RATE, # Hyperparameter grid search defined above\n",
    "        \"gamma\": 0.50, # This can have a big impact on the result and needs to be properly tuned (range is 0 to 1)\n",
    "        \"observation_filter\": \"MeanStdFilter\",\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": FC_SIZE, # Hyperparameter grid search defined above\n",
    "        },\n",
    "        \"sgd_minibatch_size\": MINIBATCH_SIZE, # Hyperparameter grid search defined above\n",
    "        \"evaluation_interval\": 1, # Run evaluation on every iteration\n",
    "        \"evaluation_config\": {\n",
    "            \"env_config\": env_config_evaluation, # The dictionary we built before (only the overriding keys to use in evaluation)\n",
    "            \"explore\": False, # We don't want to explore during evaluation. All actions have to be repeatable.\n",
    "            },\n",
    "        },\n",
    "        num_samples=1, # Have one sample for each hyperparameter combination. You can have more to average out randomness.\n",
    "        keep_checkpoints_num=10, # Keep the last 2 checkpoints\n",
    "        checkpoint_freq=1, # Do a checkpoint on each iteration (slower but you can pick more finely the checkpoint to use later)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1750238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0625fba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e6c7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
